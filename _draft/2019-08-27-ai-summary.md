---
title: "인공지능 관련 요약 정리 (AI summary)"
date: 2019-06-11T12:43:00+09:00
categories:
  - IT
tags:
  - summary
  - AI
---

# 인공지능 요약정리
## 1. 기계학습
### 1. 지도학습(Supervised Learning)  :  데이터에 대한 레이블(Label)-정답-이 주어진 상태에서 컴퓨터를 학습시키는 방법이다. (정답을 알려주며 학습시키는 것.) 따라서 학습 결과를 쉽게 알 수 있다. 지도학습에는 크게 분류(classification)와 회귀(regression)가 있다.
- 분류(classification)
  - 이진 분류 : 어떤 데이터에 대해 두 가지 중 하나로 분류할 수 있는 것.
  - 다중 분류 : 어떤 데이터에 대해 여러 값 중 하나로 분류할 수 있는 것.
- 회귀(regression)
  - 어떤 데이터들의 특징(feature)을 토대로 값을 예측하는 것. 결과 값은 실수 값을 가질 수 있다. (그 값들은 연속성을 갖는다. 그래프를 생각하면 됨)

### 2. 비지도학습(Unsupervised Learning) :  데이터에 대한 레이블(Label)-정답-이 주어지지 상태에서 컴퓨터를 학습시키는 방법론이다.(정답을 따로 알려주지 않고, 비슷한 데이터들을 군집화 하는 것.)

### 3. 강화학습(Reinforcement Learning) : 에이전트가 주어진 환경(state)에 대해 행동(action)을 취하고 이로부터 보상(reward)을 얻으면서  보상(reward)을 최대화(maximize)하도록 학습을 진행하는 방법.(상과 벌이라는 보상(reward)을 주며 상을 최대화하고 벌을 최소화 하도록 학습하는 방식.) 알파고가 이 방법으로 학습 되었고, 주로 게임에서 최적의 동작을 찾는데 쓰는 학습 방식이다.

## 2. MLP(Muti Layer Perceptron; 다중신경망)
- Perceptron은 결정적인 문제를 가지고 있는데, 직선을 그려서 AND,OR 문제를 해결할 수 는 있지만, XOR 문제를 풀어낼 수 가 없다는 것이다.
-  Perceptron을 다중으로 겹쳐 이 문제를 해결했는데 이 방법이 MLP(Multi Layer Perceptron)이다.
- Perceptron은 경사하강법을 사용해 학습을 하게 되는데 MLP에서는 레이어가 복잡해질 수록 연산이 복잡해져서 현실적으로 값을 구하는 것이 불가능해 이를 해결 하기 위해서 Back propagation이라는 알고리즘이 도입되었다.
- Gradient Descent Method와 Back propagation은 다음과 같다.
### 1. Gradient Decent Method(경사하강법) 
- 비용함수의 값을 최소화시키기 위한 파라미터 값을 점진적으로 찾는 방법으로 에러를 미분하여 weight를 업데이트 하는 방법을 사용
### 2. Back-propagation(역전파) 
- 뉴럴 네트워크를 순방향으로 한번 연산을 한 다음에, 그 결과 값을 가지고, 뉴럴 네트워크를 역방향 (backward)로 계산하면서 값을 구한다는 개념이다.(네트워크 전체에 대해 반복적인 연쇄 법칙(Chain rule)을 적용하여 그라디언트(Gradient)를 계산하는 방법 중 하나이다.)
- 그러나 이 Back Propagation 역시 문제를 가지고 있었는데, 오차를 역방향(backward)로 반영하면서 계산을 해야 하는데, 레이어가 깊을 수 록 뒤에 있는 값이 앞으로 전달이 되지 않는 문제 이다. 이를 Vanishing Gradient 문제라고 한다. 
- 이는 ReLu라는 activation function (앞에서는 sigmoid 함수를 사용했다.)으로 해결이 되었다.


## 3. CNN(Convolutional Neural Network)
- CNN은 딥러닝의 한 종류로 주로 이미지를 인식하는데 사용되며 음성, 비디오 등 타임시리즈 데이타에도 사용된다.
- 인간의 시신경 구조를 모방한 기술로 이미지 프로세싱에서 시작해 글자인식, 이미지인식, 사물인식에 이르기까지 인식에 필요한 특징을 자동으로 학습 또한, 형태 변이를 효과적으로 흡수할 수 있는 알고리즘이다. (참고. 2012년 세계적인 이미지 인식 경연 대회 (ILSVRC)에서 세계 유수의 기관을 제치고 난데없이 큰 격차로 캐나다의 토론토 대학의 슈퍼비전이 우승하게 되는데 그때 사용된 방법이 CNN에 기반합니다.)
- 이미지 인식과 같은 분야에서 MLP(Multi-Layer Perceptron) 또는 multi-layered neural network를 사용하게 되면 MLP는 모든 입력이 위치와 상관없이 동일한 수준의 중요도를 갖는다고 본다. 그렇기 때문에 (이를 이용해 fully-connected neural network를 구성하게 되면) 파라미터의 크기가 엄청나게 커지는 문제가 생긴다. 이에 대한 해결책으로 탄생한 것이 바로 CNN이다.
- 전체 영역에 대해 서로 동일한 연관성(중요도)으로 처리하는 대신에 특정 범위에 한정에 처리한다면 훨씬 효과적일 것이라는 아이디어를 바탕으로 한다.
- CNN은 일반적으로 3종류의 계층을 가지고 있다.
  - Convolution layer : Convolution feature를 추출하는 layer로 의미있는 특징들을 추출하기 위한 층
  - Pooling layer : 이미지의 특성상 많은 픽셀이 존재하기 때문에 feature를 줄이기 위해 subsampling을 하는 층
  - Feedforward layer : convolution layer와 pooling layer에서 나온 feature를 이용해서 분류를 하는 층

## 4. RNN(Recurrent Neural Networks) 구조를 간단히 그리고 그 동작원리를 설명하시오.
- 사람은 생각을 처음부터 다시 시작하지 않습니다. 이 글을 읽을 때도, 각 단어를 이전 단어들에 기반을 두어 이해합니다. 전통적 신경망은 이런 일을 할 수 없습니다. 순환 신경망(RNN)은 이 주제를 다룹니다. RNN은 그 내부에 루프를 가진 네트워크입니다. 그 루프는 정보가 지속되는 것을 돕습니다.
- RNN은 음성 인식, 언어 모델, 번역, 영상에 주석 달기 등에 사용됩니다.
- 위 그림에서 A는 RNN의 한 덩어리이다. A는 input xt를 받아서 ht를 내보낸다. A를 둘러싼 반복은 다음 단계에서의 network가 이전 단계의 정보를 받는다는 것을 보여준다. RNN을 하나의 network를 계속 복사해서 순서대로 정보를 전달하는 network라고 생각하면 된다.
- 이렇게 RNN의 체인처럼 이어지는 성질은 sequence나 list로 이어지는 데이터를 다루기에 최적화된 구조의 neural network인 것이다.
- RNN은 장기 의존성 문제 (The Problem of Long-Term Dependencies)를 가지고 있습니다.
- 우리는 현재 과제를 수행하기 위해 최근 정보를 자세히 볼 필요가 있습니다. 예를 들어, ‘the clouds are in the sky’ 이 문장에서 ‘the clouds are in the’까지만 보더라도 ‘sky’라는 마지막 단어를 쉽게 유추할 수 있다는 뜻입니다. 적절한 정보와 그 정보가 필요한 곳의 거리가 가까운 경우, RNN은 과거 정보를 사용하여 학습할 수 있습니다.
- 그러나 문맥이 더 필요한 경우도 있습니다. “나는 프랑스에서 자랐습니다 …  …. …. …. 나는 유창한 프랑스어를 합니다.” 라는 문장에서 ‘프랑스어’라는 단어를 예측하려 한다고 생각해 봅시다. 최근 정보는 아마도 다음 단어가 어느 나라 말일 지를 암시할 것입니다. 그러나 만약 그것이 어떤 언어인지까지 좁히길 원하면, 더 앞쪽에 있는 프랑스라는 문맥이 필요합니다. 이는 관련 정보와 그 정보를 사용하는 지점 사이 거리가 매우 멀어질 수 있을 때만 가능합니다.
- 불행히도, 그 거리가 멀어질수록, RNN은 그 정보의 연결 방법을 배울 수 없게 됩니다.
- 이론적으로, RNN은 그런 “장기 의존성”을 다룰 수 있지만 슬프게도 실제로는 문장표현의 순서상 갭이 커질 수록 RNN은 두 정보의 문맥을 연결하기 힘들어지고 실제로 성능도 저하되게 됩니다. LSTM은 이문제를 해결하기 위해 나왔습니다.

## 5. LSTM(Long Short-Term Memory models) 구조를 간단히 그리고 그 동작원리를 설명하시오.
- 장단기 기억 네트워크(Long Short Term Memory networks)는 보통 LSTM으로 불립니다. LSTM은 장기 의존성을 학습을 수 있는 특별한 종류의 순환 신경망(RNN)입니다.  LSTM은 매우 다양한 종류의 문제들에 대해 정말 잘 동작하며 널리 사용되고 있습니다. 오랫동안 정보를 기억하는 것이 사실상 LSTM의 기본 동작입니다. 모든 RNN은 사슬 형태의 반복되는 신경망 모듈들을 가집니다. 표준 RNN에서, 이 반복되는 모듈은 한 개의 tanh 층 같은 매우 간단한 구조를 가집니다.

- LSTM 또한 사슬 같은 구조를 가집니다. 그러나 반복되는 모듈은 다른 구조를 가지고 이 모듈에는 매우 특별한 방식으로 상호작용하는 네 개의 층이 있습니다.
- 사용기호 
  - 노란색 상자 : 학습된 신경망 층  /  분홍색 원 : 벡터 덧셈 같은 요소별 연산
  - 화살표 :  한 노드 출력에서 다른 노드의 입력으로 전체 벡터 하나를 전달 
  - 합쳐지는 화살표 : 연관(concatenate)을 표시
  - 갈라지는 화살표 : 그 내용이 복사되어 다른 곳으로 보내짐을 표시
- 셀 스테이트는 LSTM의 핵심적인 부분입니다. 이 셀스테이트는 하나의 컨베이어 벨트처럼 천체 체인을 통과합니다. 이 구조를 통해서 정보는 큰 변함 없이 계속 다음 단계에 전달이 될 수 있습니다.
- LSTM은 셀 상태에 정보를 더하거나 지울 수 있습니다. 게이트라 불리는 구조들이 이 과정을 신중히 조절합니다.
- 게이트는 정보가 선택적으로 지나가게 합니다. 게이트는 시그모이드 신경망 층과 요소별 곱셈 연산으로 구성됩니다.
- 시그모이드 레이어(The sigmoid layer)는 0 혹은 1의 값을 출력합니다. 각 구성요소가 영향을 주게 될지를 결정해주는 역할을 합니다.  0이라는 값을 가지게 되면 해당 구성요소가 매래의 결과에 아무런 영향을 주지 않게 됩니다. 1이라는 값을 가지게 되면 해당 구성요소가 확실히 미래의 예측결과에 영향을 주도록 데이터가 흘러가게 합니다.
- LSTM은 셀 상태를 보호 및 제어하기 위한 세 종류의 게이트를 가집니다. 
  - forget gate layer(시그모이드 레이어)
  - input gate layer(시그모이드 레이어)
  - tanh layer
- LSTM을 단계별로 살펴보겠습니다.
  - LSTM 스텝 1  셀 스테이트에서 어떤 정보를 버릴지 선택하는 과정

  - forget gate layer는 0과 1 사이의 출력값을 가지는 ht-1과 xt를 입력 값으로 받습니다. 여기서 xt는 새로운 입력값이고 ht1은 이전 은닉층에서 입력되는 값입니다. 
  - 출력값이 1인 경우 완전히 이 값을 유지하게 되고 출력값이 0이 될 경우 완전히 이 값을 버리는 것이 됩니다. 
  - 언어 모델 예제에서, 셀 상태는 정확한 대명사가 사용될 수 있도록 현재 주어의 성별을 포함할 수도 있습니다. 우리가 새 주어를 볼 때, 우리는 이전 주어의 성별은 잊기를 원합니다.
  - LSTM 스텝 2 새로운 정보가 셀 스테이트에 저장될지를 결정하는 단계
  - 이 단계는 두 부분으로 구성됩니다. 
  - input gate layer(시그모이드 레이어) : 어떤 값을 우리가 업데이트할 지를 결정하는 역할을 합니다. 
  - ​tanh layer : 셀 스테이트에 더해질 수 있는 새로운 후보 값 Ct를 만듭니다.
  - 언어 모델 예제에서,  잊어가고 있는 이전 것을 대체하기 위해,  새 주어의 성별을 셀 상태에 더하고 싶을 수 있습니다.
  - LSTM 스텝 3 오래된 셀 스테이트(Ct-1)를 새로운 스테이트인 Ct로 업데이트
  - 앞의 단계에서 얻은 값들을 토대로 셀 스테이트를 업데이트 합니다.
  - 이전 상태 Ct-1에 ft를 곱합니다. ft는 우리가 전에 계산한 잊기 게이트 출력입니다. ft는 우리가 잊기로 결정한 것들을 잊게 만드는 역할을 합니다. 그런 다음 itCt를 더합니다. 이것이 각 상태 값을 우리가 얼만큼 갱신할지 결정한 값으로 크기 변경한(scaled) 새 후보 값들입니다.
  - 언어 모델 예제의 경우에는, 이곳이 바로 이전 주어의 성별 정보를 지우고 새 정보를 더한 곳입니다. 우리가 이전 단계들에서 결정한 대로요.
  - LSTM 스텝 4 어떤 출력값을 출력할지 결정
  - 시그모이드 층은 셀 상태에서 어떤 부분들을 출력할지 결정합니다. 그런 다음, 값이 -1과 1 사이 값을 갖도록 셀 상태를 tanh에 넣습니다. 그리고 오직 우리가 결정한 부분만 출력하도록, tanh 출력을 다시 시그모이드 게이트 출력과 곱합니다.
  - 언어 모델 예제에서, 그것은 단지 한 주어만 보았기 때문에, 그 주어가 다음에 다시 나올 때를 대비해, 한 동사에만 알맞는 정보를 출력하려 할지도 모릅니다. 예를 들어, 만약 그 주어가 다음에 다시 나오면 동사가 어떤 형태로 활용되어야 하는지 알게 하도록, 그것은 그 주어가 단수인지 복수인지 출력할 수도 있습니다.
- 이상이 표준적인 LSTM 방식입니다. 이외에도 여러 변칙 패턴들이 있습니다.


## 3. Classification과 regression의 개념
- 분류(Classification) : 주어진 데이터를 정해진 카테고리에 따라 분류하는 문제를 말합니다. 최근에 많이 사용되는 이미지 분류도 Classification 문제 중에 하나입니다. (분류는 두 개로 분류하는 이진 분류(binary classification)과 셋 이상으로 분류하는 다중 분류(multiclass classification)으로 나누어 집니다.)
- 회귀(regression) : 연속된 값을 예측하는 문제를 말합니다. 주로 어떤 패턴이나 트렌드, 경향을 예측할 때 사용됩니다. 집의 크기에 따른 매매가격 예측,  공부시간에 따른 전공 시험 점수를 예측하는 문제 등을 예로 들 수 있습니다.

## 4. Normalization의 적용과 z-score의 이해
- Feature Scaling : 데이터 값의 크기 차이가 결과에 영향을 미칠 수도 있기 때문에 전처리 과정으로 데이터의 범위을 맞춰주는 작업이 필요하다. 방법론적으로 크게 Min-Max Normalization , Standardization (Z-score Normalization) 두가지가 있다.
- 정규화(Normalization) :  Min-Max Normalization은 최대 최소값을 사용하여 0과 1사의 값으로 변환한다.
- 표준화(standardization) : 표준화점수(z-score)는 평균이 0, 표준편차가 1인 표준정규분포로 환산하는 것(음수면 평균이하, 양수이면 평균이상)

## 9. 모델의 성능평가 방법의 이해
http://rstudio-pubs-static.s3.amazonaws.com/408280_b57fa3f6221b4326a3a528e5e66cfb21.html
https://sumniya.tistory.com/26
- 평가의 관점에서 보면, 모델을 다음과 같은 그룹으로 나눌 수 있을 것이다.
분류(Classification), 스코어링(Scoring), 확률 추정(Probability estimation), 순위(Ranking), 군집화(Clustering)
그중 분류 모델의 평가는 다음과 같다.

True Positive(TP) : True인 정답을 True라고 예측 (정답)
False Positive(FP) : False인 정답을 True라고 예측 (오답)
False Negative(FN) : True인 정답을 False라고 예측 (오답)
True Negative(TN) : False인 정답을 False라고 예측 (정답)
정밀도(precision)는 모델이 True라고 분류한 것 중에서 실제 True인 것의 비율 TP/(TP+FP),
재현률(Recall)은 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율 TP/(TP+FN)
정확도(Accuracy)는 가장 직관적으로 모델의 성능을 나타낼 수 있는 평가 지표로 전체 데이터중 맞춘 비율입니다.  하지만, 여기서 고려해야하는 것이 있습니다. 바로 domain의 편중(bias)입니다 불균형한 데이터에는 적절하지 않습니다. 따라서 이를 보완할 지표가 필요합니다. (TP+TN)/(TP+FN+FP+TN)
F1 score는 Precision과 Recall의 조화평균입니다. 데이터가 불균형 구조일 때, 모델의 성능을 정확하게 평가할 수 있으며, 성능을 하나의 숫자로 표현할 수 있습니다. (참고. 기하학적으로 봤을 때, 단순 평균이라기보다는 작은 길이 쪽으로 치우치게 된, 그러면서 작은 쪽보다도 작은 평균이 도출됩니다. 이렇게 조화평균을 이용하면 산술평균을 이용하는 것보다, 큰 비중이 끼치는 bias가 줄어든다고 볼 수 있습니다.)


## 10. Linear Regression cost function의 이해
https://m.blog.naver.com/PostView.nhn?blogId=complusblog&logNo=221238399644&proxyReferer=https%3A%2F%2Fwww.google.com%2F
선형회귀(Linear Regression)는 데이터의 분포를 가장 잘 설명할 수 있는 함수(데이터들을 관통하는 하나의 직선)를 찾아내는 머신러닝 알고리즘이다.
직선을 표현할 수 있는 함수는 다음과 같다.

직선은 W라는 기울기(weight)와 b라고하는 bias를 갖는 함수로 표현할 수 있다. Linear Regression의 목표는 W값과 b값을 찾아내는 것이라고 할 수 있다. W와 b 값에 따라 다양한 직선이 그려질 수 있다. 그 직선들 가운데 주어진 데이터를 가장 잘 표현하는 직선을 찾는 것이 Linear regression 알고리즘이 하는 일이다.
Cost function 혹은 Loss function이라고 하는 함수를 이용해서  W, b 값이 학습 데이터를 얼마나 잘 표현하는지 계량한다. (실제 학습 데이터에서의 y 값과 직선이 예측한 y 값의 차이를 이용) 직선이 예상한 값과 실제 데이터의 값이 얼마나 다른지 살펴본다. 다음과 같이 정의할 수 있다.

m개의 학습 데이터가 주어졌을 때, 각 학습 데이터에 대해서 현재 직선이 예측한 값(H(x))과 실제 학습 데이터에서의 값 (y)의 차이를 제곱한 값의 평균이다. 
제곱을 씌우는 이유는 우선 0보다 크게 만들기 위함이다.
두 번째 이유는 차이가 클 수록 더 큰 페널티를 주기 위함이다. (제곱을 하기 때문에 차이가 두 배 더 크면 4배의 페널티를 받게 된다.)
(편의를 위해 Cost값을 Parameter에 대한 함수로 정의를 할 수 있다.)

설명의 편의를 위해 bias인 b 값을 0으로 고정하고 W값의 변화에 따라 cost(w, 0)을 그려보면 다음과 같은 모양을 갖는다.

이 함수를 보면 W의 값이 1일 때 cost가 최저가 됨을 알 수 있다. 즉, 우리가 찾아야하는 값은 W=1이 되며 이를 직선 함수로 나타내면 y = 1 * x 다시말해서 y = x 가 되는 것이다.
Gradient Descent 알고리즘 : 경사도 내려가기 알고리즘이라고도 부른다.
임의의 W값이 있을 때, Cost Function에서 해당 W값이 위치하는 지점의 기울기를 보고, W를 증가시키면서 움직여야하는지 W를 감소시키면서 움직여야하는지가 결정된다. 위 그림에서는 기울기가 음수이기 때문에 W값을 증가시키면 점점 cost 값이 낮아짐을 알 수 있다.
그렇게 W값을 늘려가다보면 W=1 근처에 도달하게 되고, 어느 순간 다시 W 값이 증가하게 되어 w = 1 근처가 가장 작은 cost를 갖는 다는 것을 알 수 있게 된다. 이런 알고리즘은 Gradient Descent 알고리즘이라고한다.
현재 W값을 기준으로 cost 함수를 W에 대해서 미분하면 기울기가 나온다. 여기에 Learning rate라고 하는 알파값을 곱해서 나온 값을 이용해 다음 w값을 계산해낸다. 이를 여러번 반복하다보면 기울기가 최저값 부근에서 0에 수렴하게 되고, 결국 W값이 cost(W)를 최소화하는 인근에 수렴하게 된다.

## 11. KNN의 이해
-  K-최근접이웃(K-Nearest Neighbor, KNN)  :  분류문제에 쓰이는 대표적이면서 간단한 알고리즘이다. 얼굴 인식, 개인 영화 추천, 단백질 및 질병 추출을 위한 유전자 데이터 패턴 식별 등에 활용된다. 범주를 알지 못하는 특정 레코드 혹은 예제의 범주를 예측하는데 쓰인다. 이에 대한 전제는 class(범주)를 포함하는 train data가 충분히 존재해야한다는 점이다. KNN에서 K는 갯수를 뜻한다. 따라서 K는 1부터 어느 정수나 가능하다. 예를 들어 K가 1이라고 한다면, 가장 가까운 범주를 찾게 되고. K가 3이라고 한다면 가장 가까운 세가지 지점을 찾아서 다수결로 범주를 결정하게 된다. 가장 가까운 세 개 지점의 범주가 A, B, B라면 B라고 판단. 1-NN을 제외한 KNN은 주변 이웃의 분포에 따라 예측 결과가 충분히 달라질 수 있습니다. 최선의 k값을 선택하는 것은 데이터마다 다르게 접근해야합니다. 일반적으로 k 값이 커질수록 분류에서 이상치의 영향이 줄어들지만 분류자체를 못하게 되는 상황이 발생합니다. k가 작을 경우 데이터의 지역적 특성을 지나치게 반영하게 됩니다(overfitting). 반대로 매우 클 경우 모델이 과하게 정규화되는 경향이 있습니다(underfitting). 일반적으로는 총데이터의 제곱근값을 사용
학습 절차가 없습니다. 그도 그럴 것이 새로운 데이터가 들어왔을 때, 그제야 기존 데이터 사이의 거리를 재서 이웃들을 뽑기 때문입니다. 그래서 KNN을 모델을 별도로 구축하지 않는다는 뜻으로 게으른 모델(Lazy model)이라고 부르는 사람도 있습니다.
장점 : KNN은 학습데이터 내에 끼어있는 노이즈의 영향을 크게 받지 않으며 학습데이터 수가 많다면 꽤 효과적인 알고리즘이라고 합니다. 
단점 : 최적 이웃의 수(k)와 어떤 거리척도가 분석에 적합한지 불분명해 데이터 각각의 특성에 맞게 연구자가 임의로 선정해야 하는 단점이 있습니다. 또 새로운 관측치와 각각의 학습 데이터 사이의 거리를 전부 측정해야 하므로 계산 시간이 오래 걸리는 한계점이 존재합니다.

## 12. k-means의 이해
https://ratsgo.github.io/machine%20learning/2017/04/19/KC/
- K-평균 군집화(K-means clustering) : 대표적인 분리형 군집화 알고리즘 가운데 하나입니다. 각 군집은 하나의 중심(centroid)을 가집니다. 각 개체는 가장 가까운 중심에 할당되며, 같은 중심에 할당된 개체들이 모여 하나의 군집을 형성합니다. 사용자가 사전에 군집 수(k)를 정해야 합니다.
1. K개의 중심을 랜덤하게 선택
2. 모든 개체를 가장 가까운 중심점에 할당
3. 군집들의 중심을 재선택
4. 2-3 반복 (더이상 변경이 없가나 정해진 반복수까지)
단점 : 랜덤하게 할당되는 초기값에 따라 원하는 결과가 나오지 않을 수 있다. 군집의 크기 또는 밀도, 가 다를 경우 제대로 작동하지 않을 수 있다. 데이터의 분포가 특이할 경우에도 군집이 잘 이루어지지 않는다.
장점 : 개념과 구현이 간단하고 실행 속도가 빠르다. 특정데이터에서는 매우 좋은 성능을 보여준다.

## 13. agglomerative clustering의 이해
http://blog.naver.com/PostView.nhn?blogId=samsjang&logNo=221019280298&categoryNo=0&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView
- 계층적 군집화(hierararchical clustering) : K-means clustering과 달리 계층적 클러스터링은 최초에 클러스터의 개수를 가정할 필요가 없다는 장점이 있고 응집형(agglomerative)과 분리형(divisive)이 있다.

응집형은 주어진 데이터에서 개별 데이터 하나하나를 독립된 하나의 클러스터로 가정하고 이들을 병합하여 상위단계 클러스터를 구성하고, 이렇게 구성된 클러스터를 병합하는 과정을 반복하여 전체를 멤버로 하는 하나의 클러스터로 구성하는 방법이다.(Bottom Up)
(참고. 분리형은 응집형과 반대로 진행되며 데이터 전체를 멤버로 하는 하나의 클러스터에서 시작하여 개별 데이터로 분리해 나가는 식으로 구성하는 방법이다.(Top Down) )
대표적인 알고리즘은 단순연결과 완전연결이 있다.
단순연결 기법은 2개의 클러스터에서 각각에 속하는 멤버 사이의 거리가 가장 가까운 거리를 모두 계산하고, 가장 작은 값을 가지는 2개의 클러스터를 병합하여 상위 단계 클러스터를 구성하는 방법이며,
완전연결 기법은 2개의 클러스터에서 각각에 속하는 멤버 사이의 거리가 가장 먼 거리를 모두 계산하고, 가장 작은 값을 가지는 2개의 클러스터를 병합하여 상위 단계 클러스터를 구성하는 방법이다.

## 14. Decision tree 생성원리
https://swalloow.github.io/decison-randomforest
-  의사결정트리(Decision Trees) : 전형적인 분류 모델이며 매우 직관적인 방법 중 하나입니다. 다른 모델들과는 다르게 결과물이 시각적으로 읽히기 쉬운형태로 나타나는 것이 장점이기 때문에, (대출을 원하는 사람의 신용평가를 하고 싶을 때, 독버섯과 버섯을 분류하고 싶을 때 등) 실질적으로 분류하는 경우에 자주 사용됩니다. 또한 예측할 때 사용하는 프로세스가 명백하며, 숫자형 / 범주형 데이터를 동시에 다룰 수 있습니다. 그리고 특정 변수의 값이 누락되어도 사용할 수 있습니다.
의사결정트리를 만들기 위해서는 먼저 어떤 질문을 할 것인지, 어떤 순서로 질문을 할 것인지 정해야 합니다. 가장 좋은 방법은 예측하려는 대상에 대해 가장 많은 정보를 담고 있는 질문을 고르는 것이 좋습니다. 이러한 ‘얼마만큼의 정보를 담고 있는가’를 엔트로피(entropy) 라고 합니다. 엔트로피가 큰 질문에서부터 작은 순으로 트리생성합니다.
단점은 오버피팅(Overfitting)되기 쉽다는 것입니다. 즉, 학습 데이터에 지나치게 최적화되어 일반화가 어렵다는 말입니다. (새로운 데이터에 유연하지 못함)
이러한 오버피팅을 방지할 수 있는 대표적인 방법 중 하나가 바로 앙상블 기법을 적용한 랜덤포레스트 입니다.

## 15. Random forest의 이해
- 랜덤포레스트란 여러 개의 의사결정트리를 만들고, 다수결로 결과를 결정하는 방법입니다. 수 백개에서 수 만개까지 트리노드를 생성할 수 있고 이렇게 여러 개의 트리를 통해 투표를 해서 오버피팅이 생길 경우에 대비할 수 있습니다.
(랜덤포레스트에서는 데이터를 bootstrap 해서 포레스트를 구성합니다. bootstrap aggregating 또는 begging 이라고 하는데,) 전체 데이터를 전부 이용해서 학습시키는 것이 아니라 샘플의 결과물을 각 트리의 입력 값으로 넣어 학습하는 방식입니다. 이렇게 하면 각 트리가 서로 다른 데이터로 구축되기 때문에 랜덤성이 생기게 됩니다.  또, 남아있는 모든 변수 중에서 최적의 변수를 선택하는 것이 아니라 변수 중 일부만 선택하고 그 일부 중에서 최적의 변수를 선택하는 것입니다. 이러한 방식을 앙상블 기법(ensemble learning) 이라고 합니다.
장점 :  샘플링되지 않은 데이터를 테스트 데이터로 이용할 수 있기 때문에 데이터 전체를 학습에 사용할 수 있으며, 의사결정트리에 비해 일반화도 적용될 수 있습니다.
단점 : 데이터 셋이 비교적 적은 경우 성능이 떨어짐

## 16. Mapreduce의 원리와 hadoop의 이해
https://12bme.tistory.com/154
https://www.joinc.co.kr/w/man/12/aws/bigdata/Fundamentals3
아파치 하둡(Hadoop)은 대량의 데이터를 용이하게 처리하기 위해서 분산 컴퓨터 네트워크 기술을 사용하는 오픈 소스 소프트웨어 유틸리티들의 모음이다. 수천개의 노드에서 실행 할 수 있으며, 테라바이트 혹은 페타바이트 단위의 데이터를 저장하고 처리 할 수 있다. 하둡의 핵심은 HDFS로 알려진 분산 파일 저장소와 분산 프로그래밍 모델인 MapReduce로 구성된다. 하둡은 파일을 블록으로 분할해서 클러스터의 노드에 분산하고, 코드를 노드로 전송해서 병렬로 데이터를 처리한다.
하둡은 다수의 노드로 구성되므로 장애가 발생해도 시스템 중단 없이 운영되도록 구성이 되며, 치명적인 오류로의 확산을 막는다. (하둡의 핵심 철학: 코드(가벼움)를 데이터(무거움)가 있는 곳으로 보낸다.)
- 맵리듀스(MapReduce)는 구글에서 2004년 발표한 대용량 데이터를 처리를 위한 분산 프로그래밍 모델
맵리듀스는 맵(Map) 단계와 리듀스(Reduce) 단계로 처리 과정을 나누어 작업
Map은 흩어져 잇는 데이터를 Key, Value의 형태로 연관성 있는 데이터 분류로 묶는 작업이며
Reduce는 Map화한 작업 중 중복 데이터를 제거하고 원하는 데이터를 추출하는 작업이다

Map은 데이터 원천을 key와 value의 형태로 연관성있는 데이터 분류로 묶는 작업
Reduce는 Map화한 작업 중 중복데이터를 제거하고 원하는 데이터를 추출하는 작업
셔플링: 처리된 결과를 여러 머신에 이리저리 재배치하는 모습
mapping단계: 데이터를 받아 각 데이터 항목을 mapper로 전달, mapper는 입력데이터를 필터링하고 reducer가 처리할 수 있는 형태로 변형
reducing단계: mapper로부터 결과값을 받아 처리, reducer는 값을 받아 통합
 
 

