---
title: "인공지능 관련 요약 정리 (AI summary)"
date: 2019-06-11T12:43:00+09:00
categories:
  - IT
tags:
  - summary
  - AI
---

# 인공지능 요약정리
## 1. 기계학습
1. 지도학습(Supervised Learning)  :  데이터에 대한 레이블(Label)-정답-이 주어진 상태에서 컴퓨터를 학습시키는 방법이다. (정답을 알려주며 학습시키는 것.) 따라서 학습 결과를 쉽게 알 수 있다. 지도학습에는 크게 분류(classification)와 회귀(regression)가 있다.
  - 분류(classification)
    - 이진 분류 : 어떤 데이터에 대해 두 가지 중 하나로 분류할 수 있는 것.
    - 다중 분류 : 어떤 데이터에 대해 여러 값 중 하나로 분류할 수 있는 것.
  - 회귀(regression)
    - 어떤 데이터들의 특징(feature)을 토대로 값을 예측하는 것. 결과 값은 실수 값을 가질 수 있다. (그 값들은 연속성을 갖는다. 그래프를 생각하면 됨)

2. 비지도학습(Unsupervised Learning) :  데이터에 대한 레이블(Label)-정답-이 주어지지 상태에서 컴퓨터를 학습시키는 방법론이다.(정답을 따로 알려주지 않고, 비슷한 데이터들을 군집화 하는 것.)

3. 강화학습(Reinforcement Learning) : 에이전트가 주어진 환경(state)에 대해 행동(action)을 취하고 이로부터 보상(reward)을 얻으면서  보상(reward)을 최대화(maximize)하도록 학습을 진행하는 방법.(상과 벌이라는 보상(reward)을 주며 상을 최대화하고 벌을 최소화 하도록 학습하는 방식.) 알파고가 이 방법으로 학습 되었고, 주로 게임에서 최적의 동작을 찾는데 쓰는 학습 방식이다.

## 2. MLP 의 Gradient Descent Method 와 Back-propagation 알고리즘
Perceptron은 결정적인 문제를 가지고 있는데, 직선을 그려서 AND,OR 문제를 해결할 수 는 있지만,  XOR 문제를 풀어낼 수 가 없다는 것이다. Perceptron을 다중으로 겹쳐 이 문제를 해결했는데 이 방법이 MLP(Multi Layer Perceptron)이다.  Perceptron은 경사하강법을 사용해 학습을 하게 되는데 MLP에서는 레이어가 복잡해질 수 록, 연산이 복잡해져서 현실적으로 값을 구하는 것이 불가능해 이를 해결 하기 위해서 Back propagation이라는 알고리즘이 도입되었다.  Gradient Descent Method와 Back propagation은 다음과 같다.
가. Gradient Decent Method(경사하강법) : 비용함수의 값을 최소화시키기 위한 파라미터 값을 점진적으로 찾는 방법으로 에러를 미분하여 weight를 업데이트 하는 방법을 사용
나. Back-propagation(역전파) :  뉴럴 네트워크를 순방향으로 한번 연산을 한 다음에, 그 결과 값을 가지고, 뉴럴 네트워크를 역방향 (backward)로 계산하면서 값을 구한다는 개념이다.(네트워크 전체에 대해 반복적인 연쇄 법칙(Chain rule)을 적용하여 그라디언트(Gradient)를 계산하는 방법 중 하나이다.)
그러나 이 Back Propagation 역시 문제를 가지고 있었는데, 오차를 역방향(backward)로 반영하면서 계산을 해야 하는데, 레이어가 깊을 수 록 뒤에 있는 값이 앞으로 전달이 되지 않는 문제 이다. 이를 Vanishing Gradient 문제라고 한다. 이는 ReLu라는 activation function (앞에서는 sigmoid 함수를 사용했다.)으로 해결이 되었다.


## 3. CNN(Convolutional Neural Network)구조를 간단히 그리고 그 동작원리를 설명하시오.
CNN은 딥러닝의 한 종류로 주로 이미지를 인식하는데 사용되며 음성, 비디오 등 타임시리즈 데이타에도 사용된다.
인간의 시신경 구조를 모방한 기술로 이미지 프로세싱에서 시작해 글자인식, 이미지인식, 사물인식에 이르기까지 인식에 필요한 특징을 자동으로 학습 또한, 형태 변이를 효과적으로 흡수할 수 있는 알고리즘이다.
(참고. 2012년 세계적인 이미지 인식 경연 대회 (ILSVRC)에서 세계 유수의 기관을 제치고 난데없이 큰 격차로 캐나다의 토론토 대학의 슈퍼비전이 우승하게 되는데 그때 사용된 방법이 CNN에 기반합니다.)
이미지 인식과 같은 분야에서 MLP(Multi-Layer Perceptron) 또는 multi-layered neural network를 사용하게 되면 MLP는 모든 입력이 위치와 상관없이 동일한 수준의 중요도를 갖는다고 본다. 그렇기 때문에 (이를 이용해 fully-connected neural network를 구성하게 되면) 파라미터의 크기가 엄청나게 커지는 문제가 생긴다. 이에 대한 해결책으로 탄생한 것이 바로 CNN이다.
전체 영역에 대해 서로 동일한 연관성(중요도)으로 처리하는 대신에 특정 범위에 한정에 처리한다면 훨씬 효과적일 것이라는 아이디어를 바탕으로 한다.
CNN은 일반적으로 3종류의 계층을 가지고 있다.
Convolution layer : Convolution feature를 추출하는 layer로 의미있는 특징들을 추출하기 위한 층
Pooling layer : 이미지의 특성상 많은 픽셀이 존재하기 때문에 feature를 줄이기 위해 subsampling을 하는 층
Feedforward layer : convolution layer와 pooling layer에서 나온 feature를 이용해서 분류를 하는 층

## 4. RNN(Recurrent Neural Networks) 구조를 간단히 그리고 그 동작원리를 설명하시오.
사람은 생각을 처음부터 다시 시작하지 않습니다. 이 글을 읽을 때도, 각 단어를 이전 단어들에 기반을 두어 이해합니다. 전통적 신경망은 이런 일을 할 수 없습니다. 순환 신경망(RNN)은 이 주제를 다룹니다. RNN은 그 내부에 루프를 가진 네트워크입니다. 그 루프는 정보가 지속되는 것을 돕습니다.
RNN은 음성 인식, 언어 모델, 번역, 영상에 주석 달기 등에 사용됩니다.
위 그림에서 A는 RNN의 한 덩어리이다. A는 input xt를 받아서 ht를 내보낸다. A를 둘러싼 반복은 다음 단계에서의 network가 이전 단계의 정보를 받는다는 것을 보여준다. RNN을 하나의 network를 계속 복사해서 순서대로 정보를 전달하는 network라고 생각하면 된다.
이렇게 RNN의 체인처럼 이어지는 성질은 sequence나 list로 이어지는 데이터를 다루기에 최적화된 구조의 neural network인 것이다.
RNN은 장기 의존성 문제 (The Problem of Long-Term Dependencies)를 가지고 있습니다.
우리는 현재 과제를 수행하기 위해 최근 정보를 자세히 볼 필요가 있습니다. 예를 들어, ‘the clouds are in the sky’ 이 문장에서 ‘the clouds are in the’까지만 보더라도 ‘sky’라는 마지막 단어를 쉽게 유추할 수 있다는 뜻입니다. 적절한 정보와 그 정보가 필요한 곳의 거리가 가까운 경우, RNN은 과거 정보를 사용하여 학습할 수 있습니다.
그러나 문맥이 더 필요한 경우도 있습니다. “나는 프랑스에서 자랐습니다 …  …. …. …. 나는 유창한 프랑스어를 합니다.” 라는 문장에서 ‘프랑스어’라는 단어를 예측하려 한다고 생각해 봅시다. 최근 정보는 아마도 다음 단어가 어느 나라 말일 지를 암시할 것입니다. 그러나 만약 그것이 어떤 언어인지까지 좁히길 원하면, 더 앞쪽에 있는 프랑스라는 문맥이 필요합니다. 이는 관련 정보와 그 정보를 사용하는 지점 사이 거리가 매우 멀어질 수 있을 때만 가능합니다.
불행히도, 그 거리가 멀어질수록, RNN은 그 정보의 연결 방법을 배울 수 없게 됩니다.
이론적으로, RNN은 그런 “장기 의존성”을 다룰 수 있지만 슬프게도 실제로는 문장표현의 순서상 갭이 커질 수록 RNN은 두 정보의 문맥을 연결하기 힘들어지고 실제로 성능도 저하되게 됩니다. LSTM은 이문제를 해결하기 위해 나왔습니다.

## 5. LSTM(Long Short-Term Memory models) 구조를 간단히 그리고 그 동작원리를 설명하시오.
장단기 기억 네트워크(Long Short Term Memory networks)는 보통 LSTM으로 불립니다. LSTM은 장기 의존성을 학습을 수 있는 특별한 종류의 순환 신경망(RNN)입니다.  LSTM은 매우 다양한 종류의 문제들에 대해 정말 잘 동작하며 널리 사용되고 있습니다. 오랫동안 정보를 기억하는 것이 사실상 LSTM의 기본 동작입니다. 모든 RNN은 사슬 형태의 반복되는 신경망 모듈들을 가집니다. 표준 RNN에서, 이 반복되는 모듈은 한 개의 tanh 층 같은 매우 간단한 구조를 가집니다.

LSTM 또한 사슬 같은 구조를 가집니다. 그러나 반복되는 모듈은 다른 구조를 가지고 이 모듈에는 매우 특별한 방식으로 상호작용하는 네 개의 층이 있습니다.
사용기호 
노란색 상자 : 학습된 신경망 층  /  분홍색 원 : 벡터 덧셈 같은 요소별 연산
화살표 :  한 노드 출력에서 다른 노드의 입력으로 전체 벡터 하나를 전달 
합쳐지는 화살표 : 연관(concatenate)을 표시
갈라지는 화살표 : 그 내용이 복사되어 다른 곳으로 보내짐을 표시
셀 스테이트는 LSTM의 핵심적인 부분입니다. 이 셀스테이트는 하나의 컨베이어 벨트처럼 천체 체인을 통과합니다. 이 구조를 통해서 정보는 큰 변함 없이 계속 다음 단계에 전달이 될 수 있습니다.
LSTM은 셀 상태에 정보를 더하거나 지울 수 있습니다. 게이트라 불리는 구조들이 이 과정을 신중히 조절합니다.
게이트는 정보가 선택적으로 지나가게 합니다. 게이트는 시그모이드 신경망 층과 요소별 곱셈 연산으로 구성됩니다.
시그모이드 레이어(The sigmoid layer)는 0 혹은 1의 값을 출력합니다. 각 구성요소가 영향을 주게 될지를 결정해주는 역할을 합니다.  0이라는 값을 가지게 되면 해당 구성요소가 매래의 결과에 아무런 영향을 주지 않게 됩니다. 1이라는 값을 가지게 되면 해당 구성요소가 확실히 미래의 예측결과에 영향을 주도록 데이터가 흘러가게 합니다.
LSTM은 셀 상태를 보호 및 제어하기 위한 세 종류의 게이트를 가집니다. 
forget gate layer(시그모이드 레이어)
input gate layer(시그모이드 레이어)
tanh layer
LSTM을 단계별로 살펴보겠습니다.
LSTM 스텝 1  셀 스테이트에서 어떤 정보를 버릴지 선택하는 과정

forget gate layer는 0과 1 사이의 출력값을 가지는 ht-1과 xt를 입력 값으로 받습니다. 여기서 xt는 새로운 입력값이고 ht1은 이전 은닉층에서 입력되는 값입니다. 
출력값이 1인 경우 완전히 이 값을 유지하게 되고 출력값이 0이 될 경우 완전히 이 값을 버리는 것이 됩니다. 
언어 모델 예제에서, 셀 상태는 정확한 대명사가 사용될 수 있도록 현재 주어의 성별을 포함할 수도 있습니다. 우리가 새 주어를 볼 때, 우리는 이전 주어의 성별은 잊기를 원합니다.
LSTM 스텝 2 새로운 정보가 셀 스테이트에 저장될지를 결정하는 단계
이 단계는 두 부분으로 구성됩니다. 
input gate layer(시그모이드 레이어) : 어떤 값을 우리가 업데이트할 지를 결정하는 역할을 합니다. 
​tanh layer : 셀 스테이트에 더해질 수 있는 새로운 후보 값 Ct를 만듭니다.
언어 모델 예제에서,  잊어가고 있는 이전 것을 대체하기 위해,  새 주어의 성별을 셀 상태에 더하고 싶을 수 있습니다.
LSTM 스텝 3 오래된 셀 스테이트(Ct-1)를 새로운 스테이트인 Ct로 업데이트
앞의 단계에서 얻은 값들을 토대로 셀 스테이트를 업데이트 합니다.
이전 상태 Ct-1에 ft를 곱합니다. ft는 우리가 전에 계산한 잊기 게이트 출력입니다. ft는 우리가 잊기로 결정한 것들을 잊게 만드는 역할을 합니다. 그런 다음 itCt를 더합니다. 이것이 각 상태 값을 우리가 얼만큼 갱신할지 결정한 값으로 크기 변경한(scaled) 새 후보 값들입니다.
언어 모델 예제의 경우에는, 이곳이 바로 이전 주어의 성별 정보를 지우고 새 정보를 더한 곳입니다. 우리가 이전 단계들에서 결정한 대로요.
LSTM 스텝 4 어떤 출력값을 출력할지 결정
시그모이드 층은 셀 상태에서 어떤 부분들을 출력할지 결정합니다. 그런 다음, 값이 -1과 1 사이 값을 갖도록 셀 상태를 tanh에 넣습니다. 그리고 오직 우리가 결정한 부분만 출력하도록, tanh 출력을 다시 시그모이드 게이트 출력과 곱합니다.
언어 모델 예제에서, 그것은 단지 한 주어만 보았기 때문에, 그 주어가 다음에 다시 나올 때를 대비해, 한 동사에만 알맞는 정보를 출력하려 할지도 모릅니다. 예를 들어, 만약 그 주어가 다음에 다시 나오면 동사가 어떤 형태로 활용되어야 하는지 알게 하도록, 그것은 그 주어가 단수인지 복수인지 출력할 수도 있습니다.
이상이 표준적인 LSTM 방식입니다. 이외에도 여러 변칙 패턴들이 있습니다.
